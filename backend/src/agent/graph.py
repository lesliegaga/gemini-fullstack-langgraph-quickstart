import os

from agent.tools_and_schemas import SearchQueryList, Reflection, DualSearchQueryList, DualReflection
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import create_react_agent
from google.genai import Client

from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
    AmapSearchState,
)
from agent.configuration import Configuration
from agent.prompts import (
    get_current_date,
    query_writer_instructions,
    dual_query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    dual_reflection_instructions,
    answer_instructions,
    amap_searcher_instructions,
)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from agent.utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
)
import asyncio
import os
from langchain_mcp_adapters.client import MultiServerMCPClient

# å°è¯•å¯¼å…¥ tiktokenï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    print("âš ï¸ tiktoken åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨ token è®¡ç®—æ–¹æ³•")
    print("å»ºè®®å®‰è£…: pip install tiktoken")

# Qwen3æ¨¡å‹é…ç½®å¸¸é‡
QWEN3_MAX_CONTEXT_LENGTH = 20000  # qwen3_32bæ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
QWEN3_SAFE_MAX_TOKENS = 5000      # å®‰å…¨çš„æœ€å¤§è¾“å‡ºtokensï¼Œç•™å……è¶³ç©ºé—´ç»™è¾“å…¥
QWEN3_SAFE_CONTEXT_LENGTH = 18000 # å®‰å…¨çš„ä¸Šä¸‹æ–‡æ£€æŸ¥é•¿åº¦ï¼Œç•™ç¼“å†²ç©ºé—´

load_dotenv()

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Used for Google Search API
genai_client = Client(api_key=os.getenv("GEMINI_API_KEY"))

# MCPé…ç½®å¸¸é‡
MCP_HEALTH_CHECK_INTERVAL = 300  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
MCP_MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
MCP_SERVER_CONFIG = {
    "amap": {
        "command": "npx",
        "args": ["-y", "@amap/amap-maps-mcp-server"],
        "transport": "stdio",
        "env": {
            "AMAP_MAPS_API_KEY": os.getenv("AMAP_MAPS_API_KEY", "")
        }
    }
}

# MCPå®¢æˆ·ç«¯å’Œå·¥å…·å­˜å‚¨
mcp_client = None
amap_tools = None
_mcp_initialized = False  # æ ‡è®°æ˜¯å¦å·²åˆå§‹åŒ–
_mcp_last_health_check = 0  # æœ€åå¥åº·æ£€æŸ¥æ—¶é—´
_mcp_health_check_interval = MCP_HEALTH_CHECK_INTERVAL  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
_mcp_retry_count = 0  # é‡è¯•æ¬¡æ•°
_max_mcp_retries = MCP_MAX_RETRIES  # æœ€å¤§é‡è¯•æ¬¡æ•°
_mcp_init_lock = asyncio.Lock()  # å¼‚æ­¥é”ï¼Œé˜²æ­¢å¹¶å‘åˆå§‹åŒ–


async def check_mcp_health():
    """æ£€æŸ¥MCPè¿æ¥çš„å¥åº·çŠ¶æ€"""
    global mcp_client, amap_tools, _mcp_last_health_check
    
    import time
    current_time = time.time()
    
    # å¦‚æœè·ç¦»ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´å¤ªçŸ­ï¼Œè·³è¿‡æ£€æŸ¥
    if current_time - _mcp_last_health_check < _mcp_health_check_interval:
        return amap_tools is not None and len(amap_tools) > 0
    
    _mcp_last_health_check = current_time
    
    try:
        if mcp_client and amap_tools:
            # å°è¯•è·å–å·¥å…·åˆ—è¡¨æ¥éªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸
            tools = await mcp_client.get_tools()
            if tools and len(tools) > 0:
                print(f"âœ… MCPè¿æ¥å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå½“å‰æœ‰ {len(tools)} ä¸ªå·¥å…·")
                return True
            else:
                print("âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¤±è´¥ï¼šå·¥å…·åˆ—è¡¨ä¸ºç©º")
                return False
        else:
            print("âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¤±è´¥ï¼šå®¢æˆ·ç«¯æˆ–å·¥å…·æœªåˆå§‹åŒ–")
            return False
    except Exception as e:
        print(f"âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¼‚å¸¸ï¼š{e}")
        return False


async def initialize_mcp_tools():
    """åˆå§‹åŒ–MCPå·¥å…·ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
    global mcp_client, amap_tools, _mcp_initialized, _mcp_retry_count
    
    import os
    import threading
    
    current_pid = os.getpid()
    current_thread = threading.current_thread().ident
    
    # åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼šå…ˆæ£€æŸ¥çŠ¶æ€ï¼Œé¿å…ä¸å¿…è¦çš„é”ç­‰å¾…
    if _mcp_initialized:
        # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
        if await check_mcp_health():
            print(f"ğŸ”„ MCPå·¥å…·å·²åˆå§‹åŒ–ä¸”å¥åº·ï¼Œè¿”å›ç¼“å­˜çš„ {len(amap_tools)} ä¸ªå·¥å…· (PID: {current_pid})")
            return amap_tools
        else:
            print(f"âš ï¸ MCPè¿æ¥ä¸å¥åº·ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–... (PID: {current_pid})")
            _mcp_initialized = False
            _mcp_retry_count += 1
    
    print(f"ğŸ”’ å°è¯•è·å–MCPåˆå§‹åŒ–é”... (PID: {current_pid}, Thread: {current_thread})")
    
    # ä½¿ç”¨å¼‚æ­¥é”é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
    async with _mcp_init_lock:
        print(f"âœ… è·å¾—MCPåˆå§‹åŒ–é” (PID: {current_pid}, Thread: {current_thread})")
        
        # å†æ¬¡æ£€æŸ¥çŠ¶æ€ï¼Œé˜²æ­¢åœ¨ç­‰å¾…é”çš„è¿‡ç¨‹ä¸­å…¶ä»–çº¿ç¨‹å·²ç»å®Œæˆåˆå§‹åŒ–
        if _mcp_initialized:
            print(f"ğŸ”„ åœ¨é”å†…æ£€æŸ¥ï¼šMCPå·¥å…·å·²åˆå§‹åŒ–ï¼Œè¿”å›ç¼“å­˜çš„ {len(amap_tools)} ä¸ªå·¥å…· (PID: {current_pid})")
            return amap_tools
        
        # æ£€æŸ¥é‡è¯•æ¬¡æ•°
        if _mcp_retry_count >= _max_mcp_retries:
            print(f"âŒ MCPå·¥å…·åˆå§‹åŒ–å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼ˆ{_mcp_retry_count}æ¬¡ï¼‰ï¼Œåœæ­¢é‡è¯• (PID: {current_pid})")
            amap_tools = []
            _mcp_initialized = True
            return amap_tools
        
        try:
            print(f"ğŸš€ {'é‡æ–°' if _mcp_initialized else 'é¦–æ¬¡'}åˆå§‹åŒ–MCPå®¢æˆ·ç«¯... (å°è¯• {_mcp_retry_count + 1}/{_max_mcp_retries}, PID: {current_pid})")
            
            # å¦‚æœå·²æœ‰å®¢æˆ·ç«¯ï¼Œå…ˆå…³é—­
            if mcp_client:
                try:
                    await mcp_client.aclose()
                except:
                    pass
            
            mcp_client = MultiServerMCPClient(MCP_SERVER_CONFIG)
            
            # åŠ è½½é«˜å¾·MCPå·¥å…·
            amap_tools = await mcp_client.get_tools()
            _mcp_initialized = True
            _mcp_retry_count = 0  # é‡ç½®é‡è¯•è®¡æ•°
            print(f"âœ… æˆåŠŸåŠ è½½ {len(amap_tools)} ä¸ªé«˜å¾·MCPå·¥å…· (PID: {current_pid})")
            print(f"ğŸ¯ MCPåˆå§‹åŒ–å®Œæˆï¼ŒçŠ¶æ€æ ‡è®°ä¸º: {_mcp_initialized} (PID: {current_pid})")
            
        except Exception as e:
            print(f"âš ï¸ é«˜å¾·MCPå·¥å…·åŠ è½½å¤±è´¥: {e} (PID: {current_pid})")
            print("ç³»ç»Ÿå°†åœ¨æ²¡æœ‰é«˜å¾·åœ°å›¾æ”¯æŒçš„æƒ…å†µä¸‹è¿è¡Œ")
            amap_tools = []
            _mcp_initialized = True  # å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤å°è¯•
            print(f"ğŸ¯ MCPåˆå§‹åŒ–å¤±è´¥ä½†çŠ¶æ€å·²æ ‡è®°ä¸º: {_mcp_initialized} (PID: {current_pid})")
        
        return amap_tools


async def get_mcp_status():
    """è·å–MCPå·¥å…·çš„å½“å‰çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    global mcp_client, amap_tools, _mcp_initialized, _mcp_retry_count
    
    import os
    import threading
    
    current_pid = os.getpid()
    current_thread = threading.current_thread().ident
    
    status = {
        "pid": current_pid,
        "thread": current_thread,
        "initialized": _mcp_initialized,
        "retry_count": _mcp_retry_count,
        "tools_count": len(amap_tools) if amap_tools else 0,
        "client_exists": mcp_client is not None,
        "lock_locked": _mcp_init_lock.locked() if hasattr(_mcp_init_lock, 'locked') else "Unknown"
    }
    
    print(f"ğŸ“Š MCPçŠ¶æ€æŠ¥å‘Š (PID: {current_pid}):")
    for key, value in status.items():
        print(f"   {key}: {value}")
    
    return status


async def get_amap_tools():
    """è·å–é«˜å¾·MCPå·¥å…·ï¼ˆå»¶è¿ŸåŠ è½½ + å¥åº·æ£€æŸ¥ï¼‰"""
    if not _mcp_initialized:
        return await initialize_mcp_tools()
    
    # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    if await check_mcp_health():
        return amap_tools
    else:
        print("ğŸ”„ MCPè¿æ¥ä¸å¥åº·ï¼Œé‡æ–°åˆå§‹åŒ–...")
        return await initialize_mcp_tools()


class CustomReactAgent:
    """è‡ªå®šä¹‰React Agentï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥å’Œæ™ºèƒ½åœæ­¢æœºåˆ¶"""
    
    def __init__(self, llm, tools, max_context_length=QWEN3_SAFE_CONTEXT_LENGTH):
        self.llm = llm
        self.tools = tools
        self.max_context_length = max_context_length
        self.tool_map = {tool.name: tool for tool in tools}
        self.total_prompt_tokens = 0  # è·Ÿè¸ªæ€»çš„prompt tokens
        self.tokenizer = None  # å»¶è¿Ÿåˆå§‹åŒ–
        # åˆ›å»ºå¸¦å·¥å…·çš„LLMé“¾
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    async def _initialize_tokenizer_async(self):
        """å¼‚æ­¥åˆå§‹åŒ–åˆ†è¯å™¨"""
        if not TIKTOKEN_AVAILABLE:
            raise RuntimeError("tiktoken åº“æœªå®‰è£…ï¼Œæ— æ³•è®¡ç®— token æ•°é‡")
        
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡æ“ä½œ
            tokenizer = await asyncio.to_thread(tiktoken.get_encoding, "cl100k_base")
            print("âœ… tiktoken cl100k_base ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ")
            return tokenizer
        except Exception as e:
            print(f"âš ï¸ tiktoken cl100k_base ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                # å¤‡ç”¨ç¼–ç å™¨
                tokenizer = await asyncio.to_thread(tiktoken.get_encoding, "gpt2")
                print("âœ… tiktoken gpt2 ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ")
                return tokenizer
            except Exception as e2:
                print(f"âš ï¸ tiktoken gpt2 ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e2}")
                raise RuntimeError(f"tiktoken ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e2}")
    
    async def _ensure_tokenizer(self):
        """ç¡®ä¿åˆ†è¯å™¨å·²åˆå§‹åŒ–"""
        if not hasattr(self, 'tokenizer') or self.tokenizer is None:
            self.tokenizer = await self._initialize_tokenizer_async()
        return self.tokenizer
    
    def get_total_prompt_tokens(self):
        """è·å–ç´¯ç§¯çš„prompt tokensæ€»æ•°"""
        return self.total_prompt_tokens
    
    async def _generate_final_response(self, messages, reason="è¾¾åˆ°é™åˆ¶"):
        """ç”Ÿæˆæœ€ç»ˆå›ç­”çš„é€šç”¨æ–¹æ³•ï¼Œç”¨äºå¤ç”¨åœæ­¢é€»è¾‘"""
        print(f"âš ï¸ {reason}ï¼Œåœæ­¢å·¥å…·è°ƒç”¨å¹¶ç”Ÿæˆæœ€ç»ˆå›ç­”")
        
        # ä¿®æ”¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ åœæ­¢æŒ‡ä»¤
        stop_instruction = f"""æ‚¨ç°åœ¨å·²ç»{reason}ã€‚æ‚¨åº”è¯¥åœæ­¢è¿›è¡Œå·¥å…·è°ƒç”¨ï¼Œå¹¶åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯é‡æ–°æ€è€ƒï¼Œæä¾›æ‚¨è®¤ä¸ºæœ€å¯èƒ½çš„ç­”æ¡ˆã€‚è¯·åŸºäºæ‚¨è¿„ä»Šä¸ºæ­¢æ”¶é›†çš„æ‰€æœ‰ä¿¡æ¯æä¾›ä¸€ä»½å…¨é¢çš„æ€»ç»“æŠ¥å‘Šã€‚"""
        
        # æ·»åŠ åœæ­¢æŒ‡ä»¤ä½œä¸ºæ–°çš„ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=stop_instruction))
        
        # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›ç­” - ä½¿ç”¨æµå¼è¾“å‡º
        try:
            stream = self.llm.astream(messages, stream_usage=True)
            full = await anext(stream)
            async for chunk in stream:
                full += chunk
            final_response = full
        except Exception as e:
            print(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨: {e}")
            if 'full' in locals():
                print(f"ğŸ“ å½“å‰å·²ä¿å­˜çš„è¾“å‡ºç»“æœ: {full}")
            final_response = await self.llm.ainvoke(messages)

        messages.append(final_response)
        return messages
    
    async def calculate_text_tokens(self, text):
        """ä½¿ç”¨åˆ†è¯å·¥å…·è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        if not text:
            return 0
        
        text_str = str(text)
        
        # ç¡®ä¿åˆ†è¯å™¨å·²åˆå§‹åŒ–
        tokenizer = await self._ensure_tokenizer()
        
        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨ tokenizer.encode
        tokens = await asyncio.to_thread(tokenizer.encode, text_str)
        return len(tokens)
            
    
    async def calculate_messages_tokens(self, messages):
        """ä½¿ç”¨åˆ†è¯å·¥å…·è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» token æ•°é‡"""
        total_tokens = 0
        
        for message in messages:
            if hasattr(message, 'content') and message.content:
                # è®¡ç®—æ¶ˆæ¯å†…å®¹çš„ token
                content_tokens = await self.calculate_text_tokens(message.content)
                total_tokens += content_tokens
            
            # å¦‚æœæ˜¯å·¥å…·è°ƒç”¨æ¶ˆæ¯ï¼Œå°† tool_calls è½¬æ¢ä¸ºå­—ç¬¦ä¸²è®¡ç®— token
            elif hasattr(message, 'tool_calls') and message.tool_calls:
                tool_calls_str = str(message.tool_calls)
                tool_calls_tokens = await self.calculate_text_tokens(tool_calls_str)
                total_tokens += tool_calls_tokens
            else:
                raise ValueError(f"Unsupported message: {message}")
        
        return total_tokens
    
    async def ainvoke(self, input_data, config=None):
        """å¼‚æ­¥æ‰§è¡ŒReact Agenté€»è¾‘ï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥"""
        messages = input_data.get("messages", [])
        max_iterations = config.get("recursion_limit", 100) if config else 100
        
        # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                if msg.get("role") == "user":
                    formatted_messages.append(HumanMessage(content=msg["content"]))
                elif msg.get("role") == "assistant":
                    formatted_messages.append(AIMessage(content=msg["content"]))
                else:
                    formatted_messages.append(HumanMessage(content=str(msg)))
            else:
                formatted_messages.append(msg)
        
        messages = formatted_messages
        iteration = 0
        
        # åˆå§‹åŒ– token è®¡æ•°
        self.total_prompt_tokens = await self.calculate_messages_tokens(messages)
        print(f"ğŸš€ åˆå§‹æ¶ˆæ¯ token æ•°é‡: {self.total_prompt_tokens}")
        
        # æ·»åŠ æ ‡è®°å˜é‡æ¥è¿½è¸ªæ˜¯å¦éœ€è¦å¼ºåˆ¶åœæ­¢
        should_force_stop = False
        
        while iteration < max_iterations:
            iteration += 1
            
            self.total_prompt_tokens = await self.calculate_messages_tokens(messages)
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä½¿ç”¨tokenæ•°ï¼‰
            current_tokens = self.get_total_prompt_tokens()
            print(f"ğŸ”„ è¿­ä»£ {iteration}: å½“å‰ token æ•°é‡: {current_tokens}")
            
            if current_tokens > self.max_context_length:
                messages = await self._generate_final_response(
                    messages, 
                    f"è¾¾åˆ°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶: {current_tokens} > {self.max_context_length}"
                )
                break
            
            
            # è°ƒç”¨LLMè·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ - ä½¿ç”¨æµå¼è¾“å‡º
            try:
                stream = self.llm_with_tools.astream(messages, stream_usage=True)
                full = await anext(stream)
                async for chunk in stream:
                    full += chunk
                response = full
            except Exception as e:
                print(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨: {e}")
                if 'full' in locals():
                    print(f"ğŸ“ å½“å‰å·²ä¿å­˜çš„è¾“å‡ºç»“æœ: {full}")
                print(f"ğŸ“‹ å½“å‰æ¶ˆæ¯å†å²:")
                for i, msg in enumerate(messages):
                    role = getattr(msg, 'type', getattr(msg, 'role', 'unknown'))
                    msg_content = getattr(msg, 'content', str(msg))
                    
                    # å¤„ç†AIMessageChunkç­‰æ²¡æœ‰å®é™…contentçš„æ¶ˆæ¯
                    if not msg_content or msg_content.strip() == '':
                        if hasattr(msg, 'tool_calls') and msg.tool_calls:
                            content = f"[å·¥å…·è°ƒç”¨: {len(msg.tool_calls)}ä¸ªå·¥å…·]"
                        else:
                            content = "[ç©ºæ¶ˆæ¯]"
                    elif len(str(msg_content)) > 100:
                        content = f"{str(msg_content)[:50]}...{str(msg_content)[-50:]}"
                    else:
                        content = str(msg_content)
                    print(f"  {i+1}. [{role}] {content}")
                response = await self.llm_with_tools.ainvoke(messages)
            messages.append(response)
            self.total_prompt_tokens = await self.calculate_messages_tokens(messages)

            if self.get_total_prompt_tokens() > self.max_context_length:
                print(f"å½“å‰tokenæ•°é‡: {self.get_total_prompt_tokens()} è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if hasattr(response, 'tool_calls') and response.tool_calls:
                # å¤„ç†å·¥å…·è°ƒç”¨
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    tool_call_id = tool_call.get("id", "")
                    
                    if tool_name in self.tool_map:
                        try:
                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tool_map[tool_name].ainvoke(tool_args)
                            # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                            tool_message = ToolMessage(
                                content=str(tool_result).replace("\n",""),
                                tool_call_id=tool_call_id,
                                name=tool_name
                            )
                            # æ·»åŠ å·¥å…·æ¶ˆæ¯
                            messages.append(tool_message)
                        except Exception as e:
                            # å·¥å…·æ‰§è¡Œå¤±è´¥
                            error_message = f"Error executing tool {tool_name}: {str(e)}"
                            tool_message = ToolMessage(
                                content=error_message,
                                tool_call_id=tool_call_id,
                                name=tool_name
                            )
                            messages.append(tool_message)
                        
                        # é‡æ–°è®¡ç®—tokenå¹¶æ£€æŸ¥é•¿åº¦
                        self.total_prompt_tokens = await self.calculate_messages_tokens(messages)
                        if self.get_total_prompt_tokens() > self.max_context_length:
                            messages = await self._generate_final_response(
                                messages, 
                                f"å·¥å…·æ‰§è¡Œåè¾¾åˆ°æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶: {self.get_total_prompt_tokens()} > {self.max_context_length}"
                            )
                            should_force_stop = True
                            break
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                break
            
            # å¦‚æœéœ€è¦å¼ºåˆ¶åœæ­¢ï¼Œè·³å‡ºå¤–å±‚å¾ªç¯
            if should_force_stop:
                break
        
        # æ£€æŸ¥æ˜¯å¦å› ä¸ºè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°è€Œé€€å‡ºå¾ªç¯
        if iteration >= max_iterations and not should_force_stop:
            # æ£€æŸ¥æœ€åä¸€ä¸ªå“åº”æ˜¯å¦è¿˜æœ‰å·¥å…·è°ƒç”¨ï¼Œå¦‚æœæœ‰åˆ™éœ€è¦å¼ºåˆ¶åœæ­¢
            last_response = messages[-1] if messages else None
            if (last_response and hasattr(last_response, 'tool_calls') and 
                last_response.tool_calls):
                messages = await self._generate_final_response(
                    messages, 
                    f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶: {iteration}/{max_iterations}"
                )
        
        print(f"ğŸ¯ æœ€ç»ˆ token æ•°é‡: {self.get_total_prompt_tokens()}")
        return {"messages": messages}

# Nodes
def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates dual search queries based on the User's question.

    Uses Qwen3 32B to create optimized search queries for both web research and map search
    based on the User's question.

    Args:
        state: Current graph state containing the User's question
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including web_query_list and map_query_list
    """
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # init Qwen3 32B
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=QWEN3_SAFE_MAX_TOKENS,
        model="qwen3_32b",
        timeout=120,
        temperature=0.7,
        top_p=0.8,
        presence_penalty=1.0
    )
    structured_llm = llm.with_structured_output(DualSearchQueryList)

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = dual_query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    # Generate the dual search queries using streaming
    try:
        stream = structured_llm.stream(formatted_prompt, stream_usage=True)
        full = next(stream)
        for chunk in stream:
            full += chunk
        result = full
    except Exception as e:
        print(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨: {e}")
        if 'full' in locals():
            print(f"ğŸ“ å½“å‰å·²ä¿å­˜çš„è¾“å‡ºç»“æœ: {full}")
        result = structured_llm.invoke(formatted_prompt)
    
    # Convert to Query format for consistency
    web_queries = [{"query": q, "rationale": result.web_rationale} for q in result.web_queries]
    map_queries = [{"query": q, "rationale": result.map_rationale} for q in result.map_queries]
    
    return {
        "web_query_list": web_queries,
        "map_query_list": map_queries
    }


def continue_to_research(state: QueryGenerationState):
    """LangGraph node that sends the search queries to both web research and amap research nodes.

    This is used to spawn parallel research tasks for each query.
    """
    tasks = []
    task_id = 0
    
    # å¤„ç†ç½‘ç»œæœç´¢æŸ¥è¯¢
    for web_query in state["web_query_list"]:
        tasks.append(Send("web_research", {"search_query": web_query["query"], "id": int(task_id)}))
        task_id += 1
    
    # å¤„ç†åœ°å›¾æœç´¢æŸ¥è¯¢
    for map_query in state["map_query_list"]:
        tasks.append(Send("amap_research", {"search_query": map_query["query"], "id": int(task_id)}))
        task_id += 1
    
    return tasks


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using the native Google Search API tool.

    Executes a web search using the native Google Search API tool in combination with Gemini 2.0 Flash.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable, including search API settings

    Returns:
        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results
    """
    # Configure
    configurable = Configuration.from_runnable_config(config)
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=state["search_query"],
    )

    # Uses the google genai client as the langchain client doesn't return grounding metadata
    response = genai_client.models.generate_content(
        model=configurable.query_generator_model,
        contents=formatted_prompt,
        config={
            "tools": [{"google_search": {}}],
            "temperature": 0,
        },
    )
    
    # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
    if not response or not response.candidates:
        error_result = f"Google Search API è¿”å›æ— æ•ˆå“åº”\næŸ¥è¯¢ï¼š{state['search_query']}"
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": [error_result],
        }
    
    candidate = response.candidates[0]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ grounding_metadata å’Œ grounding_chunks
    if (not hasattr(candidate, "grounding_metadata") or 
        not candidate.grounding_metadata or 
        not hasattr(candidate.grounding_metadata, "grounding_chunks") or
        not candidate.grounding_metadata.grounding_chunks):
        
        # å¦‚æœæ²¡æœ‰ grounding ä¿¡æ¯ï¼Œè¿”å›åŸå§‹å“åº”æ–‡æœ¬
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": [f"æœç´¢ç»“æœï¼ˆæ— å¼•ç”¨ä¿¡æ¯ï¼‰ï¼š\n{response.text}"],
        }
    
    # resolve the urls to short urls for saving tokens and time
    resolved_urls = resolve_urls(
        candidate.grounding_metadata.grounding_chunks, state["id"]
    )
    
    # Gets the citations and adds them to the generated text
    citations = get_citations(response, resolved_urls)
    modified_text = insert_citation_markers(response.text, citations)
    sources_gathered = [item for citation in citations for item in citation["segments"]]

    return {
        "sources_gathered": sources_gathered,
        "search_query": [state["search_query"]],
        "web_research_result": [modified_text],
    }


def amap_research(state: AmapSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs location-based research using Amap MCP service.

    Executes location-based search using Amap (é«˜å¾·åœ°å›¾) MCP service with React agent.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable

    Returns:
        Dictionary with state update, including amap_research_result
    """
    try:
        search_query = state["search_query"]
        
        # æ£€æŸ¥é¢„åŠ è½½çš„å·¥å…·
        if not amap_tools:
            error_result = f"é«˜å¾·MCPå·¥å…·æœªåˆå§‹åŒ–\næŸ¥è¯¢ï¼š{search_query}"
            return {
                "search_query": [state["search_query"]],
                "amap_research_result": [error_result],
            }
        
        # åˆ›å»ºLLMæ¥ä½¿ç”¨MCPå·¥å…·
        configurable = Configuration.from_runnable_config(config)
        llm = ChatOpenAI(
            base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
            max_tokens=QWEN3_SAFE_MAX_TOKENS,
            model="qwen3_32b",
            timeout=120,
            temperature=0.7,
            top_p=0.8,
            presence_penalty=1.0
        )
        
        # ä½¿ç”¨ä¼˜åŒ–çš„é«˜å¾·æœç´¢æç¤º
        current_date = get_current_date()
        formatted_prompt = amap_searcher_instructions.format(
            search_query=search_query,
            current_date=current_date
        )
        
        # å®šä¹‰å¼‚æ­¥æ‰§è¡Œå‡½æ•°
        async def execute_amap_research():
            # åˆ›å»ºè‡ªå®šä¹‰React Agentæ¥æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥
            agent = CustomReactAgent(llm, amap_tools, max_context_length=QWEN3_SAFE_CONTEXT_LENGTH)
            
            # è°ƒç”¨è‡ªå®šä¹‰React Agentå¤„ç†æŸ¥è¯¢å¹¶å®é™…æ‰§è¡Œå·¥å…·ï¼Œè®¾ç½®é€’å½’é™åˆ¶ä¸º100
            response = await agent.ainvoke(
                {"messages": [{"role": "user", "content": formatted_prompt}]},
                config={"recursion_limit": 30}
            )
            
            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
            last_message = response["messages"][-1]
            return last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # åœ¨åŒæ­¥å‡½æ•°ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
        result_content = asyncio.run(execute_amap_research())
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_result = f"**é«˜å¾·åœ°å›¾æœç´¢ç»“æœ**\næŸ¥è¯¢ï¼š{search_query}\n\n{result_content}"
        
        return {
            "search_query": [state["search_query"]],
            "amap_research_result": [formatted_result],
        }
        
    except Exception as e:
        # å¦‚æœé«˜å¾·æœç´¢å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        error_result = f"é«˜å¾·åœ°å›¾æœç´¢å¤±è´¥ï¼š{str(e)}\næŸ¥è¯¢ï¼š{state['search_query']}"
        return {
            "search_query": [state["search_query"]],
            "amap_research_result": [error_result],
        }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    Analyzes the current summary to identify areas for further research and generates
    potential follow-up queries for both web and map search. Uses structured output to extract
    the follow-up queries in JSON format.

    Args:
        state: Current graph state containing the running summary and research topic
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including dual follow-up queries
    """
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reflection_model = state.get("reflection_model") or configurable.reflection_model

    # Format the prompt
    current_date = get_current_date()
    # åˆå¹¶ç½‘ç»œæœç´¢å’Œé«˜å¾·æœç´¢ç»“æœ
    all_research_results = state.get("web_research_result", []) + state.get("amap_research_result", [])
    
    formatted_prompt = dual_reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(all_research_results),
    )
    # init Reflection Model
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=QWEN3_SAFE_MAX_TOKENS,
        model="qwen3_32b",
        timeout=120,
        temperature=0.7,
        top_p=0.8,
        presence_penalty=1.0
    )
    # Generate reflection using streaming
    structured_reflection_llm = llm.with_structured_output(DualReflection)
    try:
        stream = structured_reflection_llm.stream(formatted_prompt, stream_usage=True)
        full = next(stream)
        for chunk in stream:
            full += chunk
        result = full
    except Exception as e:
        print(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨: {e}")
        if 'full' in locals():
            print(f"ğŸ“ å½“å‰å·²ä¿å­˜çš„è¾“å‡ºç»“æœ: {full}")
        result = structured_reflection_llm.invoke(formatted_prompt)

    # Combine web and map follow-up queries for backward compatibility
    combined_follow_up_queries = result.web_follow_up_queries + result.map_follow_up_queries

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": combined_follow_up_queries,
        "web_follow_up_queries": result.web_follow_up_queries,
        "map_follow_up_queries": result.map_follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph routing function that determines the next step in the research flow.

    Controls the research loop by deciding whether to continue gathering information
    or to finalize the summary based on the configured maximum number of research loops.

    Args:
        state: Current graph state containing the research loop count
        config: Configuration for the runnable, including max_research_loops setting

    Returns:
        String literal indicating the next node to visit or task list for follow-up research
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        tasks = []
        task_id = state["number_of_ran_queries"]
        
        # å¤„ç†ç½‘ç»œæœç´¢åç»­æŸ¥è¯¢
        for web_follow_up_query in state.get("web_follow_up_queries", []):
            tasks.append(Send(
                "web_research",
                {
                    "search_query": web_follow_up_query,
                    "id": task_id,
                }
            ))
            task_id += 1
            
        # å¤„ç†åœ°å›¾æœç´¢åç»­æŸ¥è¯¢
        for map_follow_up_query in state.get("map_follow_up_queries", []):
            tasks.append(Send(
                "amap_research",
                {
                    "search_query": map_follow_up_query,
                    "id": task_id,
                }
            ))
            task_id += 1
        
        return tasks


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph node that finalizes the research summary.

    Prepares the final output by deduplicating and formatting sources, then
    combining them with the running summary to create a well-structured
    research report with proper citations.

    Args:
        state: Current graph state containing the running summary and sources gathered

    Returns:
        Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    configurable = Configuration.from_runnable_config(config)
    answer_model = state.get("answer_model") or configurable.answer_model

    # Format the prompt
    current_date = get_current_date()
    
    # è¿‡æ»¤å¹¶å¹³è¡¡å¤„ç†æœç´¢ç»“æœ
    def filter_valid_results(results):
        """è¿‡æ»¤æ‰å¤±è´¥çš„æœç´¢ç»“æœ"""
        valid_results = []
        for result in results:
            # è¿‡æ»¤æ‰åŒ…å«é”™è¯¯ä¿¡æ¯çš„ç»“æœ
            if (result and 
                not any(error_keyword in result for error_keyword in [
                    "æœç´¢å¤±è´¥", "Error code:", "è¿”å›æ— æ•ˆå“åº”", 
                    "æœç´¢ç»“æœï¼ˆæ— å¼•ç”¨ä¿¡æ¯ï¼‰", "Google Search API", 
                    "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æœç´¢ç»“æœ", "æ— æ³•å®Œæˆ", "æŠ±æ­‰"
                ]) and
                len(result.strip()) > 50):  # ç¡®ä¿ç»“æœæœ‰å®è´¨å†…å®¹
                valid_results.append(result.strip())
        return valid_results
    
    # åˆ†åˆ«è·å–å’Œè¿‡æ»¤webæœç´¢å’Œé«˜å¾·æœç´¢ç»“æœ
    web_results = filter_valid_results(state.get("web_research_result", []))
    amap_results = filter_valid_results(state.get("amap_research_result", []))
    
    # å¹³è¡¡åˆå¹¶ç»“æœï¼Œç¡®ä¿ä¸¤ç§æœç´¢ç»“æœå¾—åˆ°å¹³ç­‰å¯¹å¾…
    balanced_results = []
    max_len = max(len(web_results), len(amap_results))
    
    for i in range(max_len):
        if i < len(web_results):
            balanced_results.append(f"**ç½‘ç»œæœç´¢å‘ç°ï¼š**\n{web_results[i]}")
        if i < len(amap_results):
            balanced_results.append(f"**åœ°å›¾ä½ç½®ä¿¡æ¯ï¼š**\n{amap_results[i]}")
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç»“æœï¼Œæ·»åŠ æç¤ºä¿¡æ¯
    if not balanced_results:
        balanced_results = ["æ ¹æ®æœç´¢ç»“æœï¼Œæœªèƒ½è·å–åˆ°è¯¦ç»†çš„ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨é€šè¿‡å…¶ä»–æ¸ é“è·å–æ›´å¤šä¿¡æ¯ã€‚"]
    
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(balanced_results),
    )

    # init Answer Model, default to Qwen3 32B
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=QWEN3_SAFE_MAX_TOKENS,
        model="qwen3_32b",
        timeout=120,
        temperature=0.7,
        top_p=0.8,
        presence_penalty=1.0
    )
    # Generate final answer using streaming
    try:
        stream = llm.stream(formatted_prompt, stream_usage=True)
        full = next(stream)
        for chunk in stream:
            full += chunk
        result = full
    except Exception as e:
        print(f"âš ï¸ æµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨: {e}")
        if 'full' in locals():
            print(f"ğŸ“ å½“å‰å·²ä¿å­˜çš„è¾“å‡ºç»“æœ: {full}")
        result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }


async def make_graph():
    """åˆ›å»ºå¹¶åˆå§‹åŒ–LangGraphå›¾ï¼ŒåŒ…æ‹¬MCPå·¥å…·çš„å¼‚æ­¥åŠ è½½ã€‚
    
    å‚è€ƒlangchain-mcp-adaptersçš„"Using with LangGraph StateGraph"ç¤ºä¾‹ã€‚
    
    Returns:
        ç¼–è¯‘å¥½çš„LangGraphå›¾å®ä¾‹
    """
    import os
    current_pid = os.getpid()
    print(f"ğŸ—ï¸ å¼€å§‹åˆ›å»ºLangGraphå›¾ (PID: {current_pid})")
    
    # ç¡®ä¿MCPå·¥å…·å·²åˆå§‹åŒ–ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    print(f"ğŸ”§ æ£€æŸ¥MCPå·¥å…·çŠ¶æ€ (PID: {current_pid})")
    await get_mcp_status()
    
    await initialize_mcp_tools()
    
    print(f"âœ… MCPå·¥å…·åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹åˆ›å»ºå›¾ (PID: {current_pid})")
    
    # åˆ›å»ºAgentå›¾
    builder = StateGraph(OverallState, config_schema=Configuration)

    # å®šä¹‰èŠ‚ç‚¹
    builder.add_node("generate_query", generate_query)
    builder.add_node("web_research", web_research)
    builder.add_node("amap_research", amap_research)
    builder.add_node("reflection", reflection)
    builder.add_node("finalize_answer", finalize_answer)

    # è®¾ç½®å…¥å£ç‚¹
    builder.add_edge(START, "generate_query")
    
    # æ·»åŠ æ¡ä»¶è¾¹ä»¥ç»§ç»­å¹¶è¡Œåˆ†æ”¯çš„æœç´¢æŸ¥è¯¢
    builder.add_conditional_edges(
        "generate_query", continue_to_research, ["web_research", "amap_research"]
    )
    
    # åæ€ç½‘ç»œæœç´¢å’Œé«˜å¾·æœç´¢
    builder.add_edge("web_research", "reflection")
    builder.add_edge("amap_research", "reflection")
    
    # è¯„ä¼°ç ”ç©¶
    builder.add_conditional_edges(
        "reflection", evaluate_research, ["web_research", "amap_research", "finalize_answer"]
    )
    
    # å®Œæˆç­”æ¡ˆ
    builder.add_edge("finalize_answer", END)

    print(f"ğŸ¯ LangGraphå›¾åˆ›å»ºå®Œæˆ (PID: {current_pid})")
    return builder.compile(
        name="pro-search-agent"
        # æ³¨æ„ï¼šé€’å½’é™åˆ¶åœ¨æ–°ç‰ˆæœ¬LangGraphä¸­é€šè¿‡å…¶ä»–æ–¹å¼è®¾ç½®
        # å½“å‰ç‰ˆæœ¬é€šè¿‡RunnableConfigåœ¨è¿è¡Œæ—¶ä¼ é€’recursion_limit
    )


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŒæ­¥å›¾å˜é‡
# ä½†æ¨èä½¿ç”¨make_graph()å‡½æ•°æ¥è·å–å›¾å®ä¾‹
graph = None
