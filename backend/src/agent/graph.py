import os

from agent.tools_and_schemas import SearchQueryList, Reflection, DualSearchQueryList, DualReflection
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import create_react_agent
from google.genai import Client

from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
    AmapSearchState,
)
from agent.configuration import Configuration
from agent.prompts import (
    get_current_date,
    query_writer_instructions,
    dual_query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    dual_reflection_instructions,
    answer_instructions,
    amap_searcher_instructions,
)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from agent.utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
)
import asyncio
import os
from langchain_mcp_adapters.client import MultiServerMCPClient
load_dotenv()

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Used for Google Search API
genai_client = Client(api_key=os.getenv("GEMINI_API_KEY"))

# MCPé…ç½®å¸¸é‡
MCP_HEALTH_CHECK_INTERVAL = 300  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
MCP_MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
MCP_SERVER_CONFIG = {
    "amap": {
        "command": "npx",
        "args": ["-y", "@amap/amap-maps-mcp-server"],
        "transport": "stdio",
        "env": {
            "AMAP_MAPS_API_KEY": os.getenv("AMAP_MAPS_API_KEY", "")
        }
    }
}

# MCPå®¢æˆ·ç«¯å’Œå·¥å…·å­˜å‚¨
mcp_client = None
amap_tools = None
_mcp_initialized = False  # æ ‡è®°æ˜¯å¦å·²åˆå§‹åŒ–
_mcp_last_health_check = 0  # æœ€åå¥åº·æ£€æŸ¥æ—¶é—´
_mcp_health_check_interval = MCP_HEALTH_CHECK_INTERVAL  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
_mcp_retry_count = 0  # é‡è¯•æ¬¡æ•°
_max_mcp_retries = MCP_MAX_RETRIES  # æœ€å¤§é‡è¯•æ¬¡æ•°


async def check_mcp_health():
    """æ£€æŸ¥MCPè¿æ¥çš„å¥åº·çŠ¶æ€"""
    global mcp_client, amap_tools, _mcp_last_health_check
    
    import time
    current_time = time.time()
    
    # å¦‚æœè·ç¦»ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´å¤ªçŸ­ï¼Œè·³è¿‡æ£€æŸ¥
    if current_time - _mcp_last_health_check < _mcp_health_check_interval:
        return amap_tools is not None and len(amap_tools) > 0
    
    _mcp_last_health_check = current_time
    
    try:
        if mcp_client and amap_tools:
            # å°è¯•è·å–å·¥å…·åˆ—è¡¨æ¥éªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸
            tools = await mcp_client.get_tools()
            if tools and len(tools) > 0:
                print(f"âœ… MCPè¿æ¥å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå½“å‰æœ‰ {len(tools)} ä¸ªå·¥å…·")
                return True
            else:
                print("âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¤±è´¥ï¼šå·¥å…·åˆ—è¡¨ä¸ºç©º")
                return False
        else:
            print("âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¤±è´¥ï¼šå®¢æˆ·ç«¯æˆ–å·¥å…·æœªåˆå§‹åŒ–")
            return False
    except Exception as e:
        print(f"âš ï¸ MCPè¿æ¥å¥åº·æ£€æŸ¥å¼‚å¸¸ï¼š{e}")
        return False


async def initialize_mcp_tools():
    """åˆå§‹åŒ–MCPå·¥å…·ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
    global mcp_client, amap_tools, _mcp_initialized, _mcp_retry_count
    
    if _mcp_initialized:
        # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
        if await check_mcp_health():
            print(f"ğŸ”„ MCPå·¥å…·å·²åˆå§‹åŒ–ä¸”å¥åº·ï¼Œè¿”å›ç¼“å­˜çš„ {len(amap_tools)} ä¸ªå·¥å…·")
            return amap_tools
        else:
            print("âš ï¸ MCPè¿æ¥ä¸å¥åº·ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–...")
            _mcp_initialized = False
            _mcp_retry_count += 1
    
    # æ£€æŸ¥é‡è¯•æ¬¡æ•°
    if _mcp_retry_count >= _max_mcp_retries:
        print(f"âŒ MCPå·¥å…·åˆå§‹åŒ–å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼ˆ{_mcp_retry_count}æ¬¡ï¼‰ï¼Œåœæ­¢é‡è¯•")
        amap_tools = []
        _mcp_initialized = True
        return amap_tools
    
    try:
        print(f"ğŸš€ {'é‡æ–°' if _mcp_initialized else 'é¦–æ¬¡'}åˆå§‹åŒ–MCPå®¢æˆ·ç«¯... (å°è¯• {_mcp_retry_count + 1}/{_max_mcp_retries})")
        
        # å¦‚æœå·²æœ‰å®¢æˆ·ç«¯ï¼Œå…ˆå…³é—­
        if mcp_client:
            try:
                await mcp_client.aclose()
            except:
                pass
        
        mcp_client = MultiServerMCPClient(
            MCP_SERVER_CONFIG
        )
        
        # åŠ è½½é«˜å¾·MCPå·¥å…·
        amap_tools = await mcp_client.get_tools()
        _mcp_initialized = True
        _mcp_retry_count = 0  # é‡ç½®é‡è¯•è®¡æ•°
        print(f"âœ… æˆåŠŸåŠ è½½ {len(amap_tools)} ä¸ªé«˜å¾·MCPå·¥å…·")
        
    except Exception as e:
        print(f"âš ï¸ é«˜å¾·MCPå·¥å…·åŠ è½½å¤±è´¥: {e}")
        print("ç³»ç»Ÿå°†åœ¨æ²¡æœ‰é«˜å¾·åœ°å›¾æ”¯æŒçš„æƒ…å†µä¸‹è¿è¡Œ")
        amap_tools = []
        _mcp_initialized = True  # å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤å°è¯•
    
    return amap_tools


async def get_amap_tools():
    """è·å–é«˜å¾·MCPå·¥å…·ï¼ˆå»¶è¿ŸåŠ è½½ + å¥åº·æ£€æŸ¥ï¼‰"""
    if not _mcp_initialized:
        return await initialize_mcp_tools()
    
    # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    if await check_mcp_health():
        return amap_tools
    else:
        print("ğŸ”„ MCPè¿æ¥ä¸å¥åº·ï¼Œé‡æ–°åˆå§‹åŒ–...")
        return await initialize_mcp_tools()


class CustomReactAgent:
    """è‡ªå®šä¹‰React Agentï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥å’Œæ™ºèƒ½åœæ­¢æœºåˆ¶"""
    
    def __init__(self, llm, tools, max_context_length=15000):
        self.llm = llm
        self.tools = tools
        self.max_context_length = max_context_length
        self.tool_map = {tool.name: tool for tool in tools}
        self.total_prompt_tokens = 0  # è·Ÿè¸ªæ€»çš„prompt tokens
    
    def get_total_prompt_tokens(self):
        """è·å–ç´¯ç§¯çš„prompt tokensæ€»æ•°"""
        return self.total_prompt_tokens
    
    def calculate_text_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        try:
            # å¯¹äº ChatOpenAIï¼Œä½¿ç”¨ get_num_tokens æ–¹æ³•
            if hasattr(self.llm, 'get_num_tokens'):
                return self.llm.get_num_tokens(text)
            # å¯¹äºå…¶ä»– LLMï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¼°ç®—ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
            else:
                # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡çº¦4ä¸ªå­—ç¬¦1ä¸ªtokenï¼Œä¸­æ–‡çº¦2ä¸ªå­—ç¬¦1ä¸ªtoken
                english_chars = sum(1 for c in text if ord(c) < 128)
                chinese_chars = len(text) - english_chars
                return english_chars // 4 + chinese_chars // 2
        except Exception:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦æ•°ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            return len(text) // 3
    
    def update_token_usage(self, response):
        """ä»LLMå“åº”ä¸­æå–å¹¶æ›´æ–°tokenä½¿ç”¨æƒ…å†µ"""
        if hasattr(response, 'usage') and response.usage:
            if hasattr(response.usage, 'total_tokens'):
                self.total_prompt_tokens = response.usage.total_tokens
        elif hasattr(response, 'response_metadata') and response.response_metadata:
            # å°è¯•ä»response_metadataä¸­è·å–tokenä¿¡æ¯
            metadata = response.response_metadata
            if 'token_usage' in metadata:
                token_usage = metadata['token_usage']
                if 'total_tokens' in token_usage:
                    self.total_prompt_tokens = token_usage['total_tokens']
        print(f"æ›´æ–°tokenä½¿ç”¨æƒ…å†µ: {self.total_prompt_tokens} usage: {response.usage if hasattr(response, 'usage') else 'None'}")
    
    def add_tool_message_tokens(self, tool_message):
        """è®¡ç®—å¹¶æ·»åŠ ToolMessageå†…å®¹çš„tokenæ•°é‡åˆ°æ€»è®¡æ•°ä¸­"""
        if hasattr(tool_message, 'content') and tool_message.content:
            content_tokens = self.calculate_text_tokens(str(tool_message.content))
            self.total_prompt_tokens += content_tokens
            print(f"ToolMessage å†…å®¹æ·»åŠ äº† {content_tokens} ä¸ªtokensï¼Œæ€»è®¡: {self.total_prompt_tokens}")
    
    async def ainvoke(self, input_data, config=None):
        """å¼‚æ­¥æ‰§è¡ŒReact Agenté€»è¾‘ï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥"""
        messages = input_data.get("messages", [])
        max_iterations = config.get("recursion_limit", 100) if config else 100
        
        # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                if msg.get("role") == "user":
                    formatted_messages.append(HumanMessage(content=msg["content"]))
                elif msg.get("role") == "assistant":
                    formatted_messages.append(AIMessage(content=msg["content"]))
                else:
                    formatted_messages.append(HumanMessage(content=str(msg)))
            else:
                formatted_messages.append(msg)
        
        messages = formatted_messages
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä½¿ç”¨tokenæ•°ï¼‰
            current_tokens = self.get_total_prompt_tokens()
            
            if current_tokens > self.max_context_length:
                # ä¿®æ”¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ åœæ­¢æŒ‡ä»¤
                stop_instruction = """æ‚¨ç°åœ¨å·²ç»è¾¾åˆ°äº†å¯ä»¥å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æ‚¨åº”è¯¥åœæ­¢è¿›è¡Œå·¥å…·è°ƒç”¨ï¼Œå¹¶åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯é‡æ–°æ€è€ƒï¼Œæä¾›æ‚¨è®¤ä¸ºæœ€å¯èƒ½çš„ç­”æ¡ˆã€‚è¯·åŸºäºæ‚¨è¿„ä»Šä¸ºæ­¢æ”¶é›†çš„æ‰€æœ‰ä¿¡æ¯æä¾›ä¸€ä»½å…¨é¢çš„æ€»ç»“æŠ¥å‘Šã€‚"""
                
                # æ·»åŠ åœæ­¢æŒ‡ä»¤ä½œä¸ºæ–°çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=stop_instruction))
                
                # è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›ç­”
                final_response = await self.llm.ainvoke(messages)
                # æ›´æ–°tokenä½¿ç”¨æƒ…å†µ
                self.update_token_usage(final_response)
                messages.append(final_response)
                break
            
            # åˆ›å»ºå¸¦å·¥å…·çš„LLMé“¾
            llm_with_tools = self.llm.bind_tools(self.tools)
            
            # è°ƒç”¨LLMè·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨
            response = await llm_with_tools.ainvoke(messages)
            # æ›´æ–°tokenä½¿ç”¨æƒ…å†µ
            self.update_token_usage(response)
            messages.append(response)

            if self.get_total_prompt_tokens() > self.max_context_length:
                print(f"å½“å‰tokenæ•°é‡: {self.get_total_prompt_tokens()} è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if hasattr(response, 'tool_calls') and response.tool_calls:
                # å¤„ç†å·¥å…·è°ƒç”¨
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]
                    tool_call_id = tool_call.get("id", "")
                    
                    if tool_name in self.tool_map:
                        try:
                            # æ‰§è¡Œå·¥å…·
                            tool_result = await self.tool_map[tool_name].ainvoke(tool_args)
                            # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                            tool_message = ToolMessage(
                                content=str(tool_result),
                                tool_call_id=tool_call_id,
                                name=tool_name
                            )
                            # æ·»åŠ å·¥å…·æ¶ˆæ¯
                            messages.append(tool_message)
                            # è®¡ç®—å¹¶æ·»åŠ ToolMessageå†…å®¹çš„tokenæ•°é‡
                            self.add_tool_message_tokens(tool_message)
                        except Exception as e:
                            # å·¥å…·æ‰§è¡Œå¤±è´¥
                            error_message = f"Error executing tool {tool_name}: {str(e)}"
                            tool_message = ToolMessage(
                                content=error_message,
                                tool_call_id=tool_call_id,
                                name=tool_name
                            )
                            messages.append(tool_message)
                            # è®¡ç®—å¹¶æ·»åŠ é”™è¯¯æ¶ˆæ¯çš„tokenæ•°é‡
                            self.add_tool_message_tokens(tool_message)
                            if self.get_total_prompt_tokens() > self.max_context_length:
                                print(f"å½“å‰tokenæ•°é‡: {self.get_total_prompt_tokens()} è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}")
                                break
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                break
        
        return {"messages": messages}

# Nodes
def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates dual search queries based on the User's question.

    Uses Qwen3 32B to create optimized search queries for both web research and map search
    based on the User's question.

    Args:
        state: Current graph state containing the User's question
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including web_query_list and map_query_list
    """
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # init Qwen3 32B
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=10000,
        model="qwen3_32b",
        timeout=120,
        temperature=1.0,
        max_retries=2,
    )
    structured_llm = llm.with_structured_output(DualSearchQueryList)

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = dual_query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    # Generate the dual search queries
    result = structured_llm.invoke(formatted_prompt)
    
    # Convert to Query format for consistency
    web_queries = [{"query": q, "rationale": result.web_rationale} for q in result.web_queries]
    map_queries = [{"query": q, "rationale": result.map_rationale} for q in result.map_queries]
    
    return {
        "web_query_list": web_queries,
        "map_query_list": map_queries
    }


def continue_to_research(state: QueryGenerationState):
    """LangGraph node that sends the search queries to both web research and amap research nodes.

    This is used to spawn parallel research tasks for each query.
    """
    tasks = []
    task_id = 0
    
    # å¤„ç†ç½‘ç»œæœç´¢æŸ¥è¯¢
    for web_query in state["web_query_list"]:
        tasks.append(Send("web_research", {"search_query": web_query["query"], "id": int(task_id)}))
        task_id += 1
    
    # å¤„ç†åœ°å›¾æœç´¢æŸ¥è¯¢
    for map_query in state["map_query_list"]:
        tasks.append(Send("amap_research", {"search_query": map_query["query"], "id": int(task_id)}))
        task_id += 1
    
    return tasks


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using the native Google Search API tool.

    Executes a web search using the native Google Search API tool in combination with Gemini 2.0 Flash.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable, including search API settings

    Returns:
        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results
    """
    # Configure
    configurable = Configuration.from_runnable_config(config)
    formatted_prompt = web_searcher_instructions.format(
        current_date=get_current_date(),
        research_topic=state["search_query"],
    )

    # Uses the google genai client as the langchain client doesn't return grounding metadata
    response = genai_client.models.generate_content(
        model=configurable.query_generator_model,
        contents=formatted_prompt,
        config={
            "tools": [{"google_search": {}}],
            "temperature": 0,
        },
    )
    
    # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
    if not response or not response.candidates:
        error_result = f"Google Search API è¿”å›æ— æ•ˆå“åº”\næŸ¥è¯¢ï¼š{state['search_query']}"
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": [error_result],
        }
    
    candidate = response.candidates[0]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ grounding_metadata å’Œ grounding_chunks
    if (not hasattr(candidate, "grounding_metadata") or 
        not candidate.grounding_metadata or 
        not hasattr(candidate.grounding_metadata, "grounding_chunks") or
        not candidate.grounding_metadata.grounding_chunks):
        
        # å¦‚æœæ²¡æœ‰ grounding ä¿¡æ¯ï¼Œè¿”å›åŸå§‹å“åº”æ–‡æœ¬
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": [f"æœç´¢ç»“æœï¼ˆæ— å¼•ç”¨ä¿¡æ¯ï¼‰ï¼š\n{response.text}"],
        }
    
    # resolve the urls to short urls for saving tokens and time
    resolved_urls = resolve_urls(
        candidate.grounding_metadata.grounding_chunks, state["id"]
    )
    
    # Gets the citations and adds them to the generated text
    citations = get_citations(response, resolved_urls)
    modified_text = insert_citation_markers(response.text, citations)
    sources_gathered = [item for citation in citations for item in citation["segments"]]

    return {
        "sources_gathered": sources_gathered,
        "search_query": [state["search_query"]],
        "web_research_result": [modified_text],
    }


def amap_research(state: AmapSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs location-based research using Amap MCP service.

    Executes location-based search using Amap (é«˜å¾·åœ°å›¾) MCP service with React agent.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable

    Returns:
        Dictionary with state update, including amap_research_result
    """
    try:
        search_query = state["search_query"]
        
        # æ£€æŸ¥é¢„åŠ è½½çš„å·¥å…·
        if not amap_tools:
            error_result = f"é«˜å¾·MCPå·¥å…·æœªåˆå§‹åŒ–\næŸ¥è¯¢ï¼š{search_query}"
            return {
                "search_query": [state["search_query"]],
                "amap_research_result": [error_result],
            }
        
        # åˆ›å»ºLLMæ¥ä½¿ç”¨MCPå·¥å…·
        configurable = Configuration.from_runnable_config(config)
        llm = ChatOpenAI(
            base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
            max_tokens=15000,
            model="qwen3_32b",
            timeout=120,
            temperature=0.1,
            max_retries=2,
        )
        
        # ä½¿ç”¨ä¼˜åŒ–çš„é«˜å¾·æœç´¢æç¤º
        current_date = get_current_date()
        formatted_prompt = amap_searcher_instructions.format(
            search_query=search_query,
            current_date=current_date
        )
        
        # å®šä¹‰å¼‚æ­¥æ‰§è¡Œå‡½æ•°
        async def execute_amap_research():
            # åˆ›å»ºè‡ªå®šä¹‰React Agentæ¥æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œæ”¯æŒä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥
            agent = CustomReactAgent(llm, amap_tools, max_context_length=15000)
            
            # è°ƒç”¨è‡ªå®šä¹‰React Agentå¤„ç†æŸ¥è¯¢å¹¶å®é™…æ‰§è¡Œå·¥å…·ï¼Œè®¾ç½®é€’å½’é™åˆ¶ä¸º100
            response = await agent.ainvoke(
                {"messages": [{"role": "user", "content": formatted_prompt}]},
                config={"recursion_limit": 100}
            )
            
            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
            last_message = response["messages"][-1]
            return last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # åœ¨åŒæ­¥å‡½æ•°ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
        result_content = asyncio.run(execute_amap_research())
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_result = f"**é«˜å¾·åœ°å›¾æœç´¢ç»“æœ**\næŸ¥è¯¢ï¼š{search_query}\n\n{result_content}"
        
        return {
            "search_query": [state["search_query"]],
            "amap_research_result": [formatted_result],
        }
        
    except Exception as e:
        # å¦‚æœé«˜å¾·æœç´¢å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        error_result = f"é«˜å¾·åœ°å›¾æœç´¢å¤±è´¥ï¼š{str(e)}\næŸ¥è¯¢ï¼š{state['search_query']}"
        return {
            "search_query": [state["search_query"]],
            "amap_research_result": [error_result],
        }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    Analyzes the current summary to identify areas for further research and generates
    potential follow-up queries for both web and map search. Uses structured output to extract
    the follow-up queries in JSON format.

    Args:
        state: Current graph state containing the running summary and research topic
        config: Configuration for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including dual follow-up queries
    """
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reflection_model = state.get("reflection_model") or configurable.reflection_model

    # Format the prompt
    current_date = get_current_date()
    # åˆå¹¶ç½‘ç»œæœç´¢å’Œé«˜å¾·æœç´¢ç»“æœ
    all_research_results = state.get("web_research_result", []) + state.get("amap_research_result", [])
    
    formatted_prompt = dual_reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(all_research_results),
    )
    # init Reflection Model
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=15000,
        model="qwen3_32b",
        timeout=120,
        temperature=1.0,
        max_retries=2,
    )
    result = llm.with_structured_output(DualReflection).invoke(formatted_prompt)

    # Combine web and map follow-up queries for backward compatibility
    combined_follow_up_queries = result.web_follow_up_queries + result.map_follow_up_queries

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": combined_follow_up_queries,
        "web_follow_up_queries": result.web_follow_up_queries,
        "map_follow_up_queries": result.map_follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph routing function that determines the next step in the research flow.

    Controls the research loop by deciding whether to continue gathering information
    or to finalize the summary based on the configured maximum number of research loops.

    Args:
        state: Current graph state containing the research loop count
        config: Configuration for the runnable, including max_research_loops setting

    Returns:
        String literal indicating the next node to visit or task list for follow-up research
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        tasks = []
        task_id = state["number_of_ran_queries"]
        
        # å¤„ç†ç½‘ç»œæœç´¢åç»­æŸ¥è¯¢
        for web_follow_up_query in state.get("web_follow_up_queries", []):
            tasks.append(Send(
                "web_research",
                {
                    "search_query": web_follow_up_query,
                    "id": task_id,
                }
            ))
            task_id += 1
            
        # å¤„ç†åœ°å›¾æœç´¢åç»­æŸ¥è¯¢
        for map_follow_up_query in state.get("map_follow_up_queries", []):
            tasks.append(Send(
                "amap_research",
                {
                    "search_query": map_follow_up_query,
                    "id": task_id,
                }
            ))
            task_id += 1
        
        return tasks


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph node that finalizes the research summary.

    Prepares the final output by deduplicating and formatting sources, then
    combining them with the running summary to create a well-structured
    research report with proper citations.

    Args:
        state: Current graph state containing the running summary and sources gathered

    Returns:
        Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    configurable = Configuration.from_runnable_config(config)
    answer_model = state.get("answer_model") or configurable.answer_model

    # Format the prompt
    current_date = get_current_date()
    # åˆå¹¶ç½‘ç»œæœç´¢å’Œé«˜å¾·æœç´¢ç»“æœ
    all_research_results = state.get("web_research_result", []) + state.get("amap_research_result", [])
    
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n---\n\n".join(all_research_results),
    )

    # init Answer Model, default to Qwen3 32B
    llm = ChatOpenAI(
        base_url="http://proxy2-search.proxy.amap.com/zjy_llm_qwen/v1",
        max_tokens=15000,
        model="qwen3_32b",
        timeout=120,
        temperature=0,
        max_retries=2,
    )
    result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }


async def make_graph():
    """åˆ›å»ºå¹¶åˆå§‹åŒ–LangGraphå›¾ï¼ŒåŒ…æ‹¬MCPå·¥å…·çš„å¼‚æ­¥åŠ è½½ã€‚
    
    å‚è€ƒlangchain-mcp-adaptersçš„"Using with LangGraph StateGraph"ç¤ºä¾‹ã€‚
    
    Returns:
        ç¼–è¯‘å¥½çš„LangGraphå›¾å®ä¾‹
    """
    # ç¡®ä¿MCPå·¥å…·å·²åˆå§‹åŒ–ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    await initialize_mcp_tools()
    
    # åˆ›å»ºAgentå›¾
    builder = StateGraph(OverallState, config_schema=Configuration)

    # å®šä¹‰èŠ‚ç‚¹
    builder.add_node("generate_query", generate_query)
    builder.add_node("web_research", web_research)
    builder.add_node("amap_research", amap_research)
    builder.add_node("reflection", reflection)
    builder.add_node("finalize_answer", finalize_answer)

    # è®¾ç½®å…¥å£ç‚¹
    builder.add_edge(START, "generate_query")
    
    # æ·»åŠ æ¡ä»¶è¾¹ä»¥ç»§ç»­å¹¶è¡Œåˆ†æ”¯çš„æœç´¢æŸ¥è¯¢
    builder.add_conditional_edges(
        "generate_query", continue_to_research, ["web_research", "amap_research"]
    )
    
    # åæ€ç½‘ç»œæœç´¢å’Œé«˜å¾·æœç´¢
    builder.add_edge("web_research", "reflection")
    builder.add_edge("amap_research", "reflection")
    
    # è¯„ä¼°ç ”ç©¶
    builder.add_conditional_edges(
        "reflection", evaluate_research, ["web_research", "amap_research", "finalize_answer"]
    )
    
    # å®Œæˆç­”æ¡ˆ
    builder.add_edge("finalize_answer", END)

    return builder.compile(
        name="pro-search-agent"
        # æ³¨æ„ï¼šé€’å½’é™åˆ¶åœ¨æ–°ç‰ˆæœ¬LangGraphä¸­é€šè¿‡å…¶ä»–æ–¹å¼è®¾ç½®
        # å½“å‰ç‰ˆæœ¬é€šè¿‡RunnableConfigåœ¨è¿è¡Œæ—¶ä¼ é€’recursion_limit
    )


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŒæ­¥å›¾å˜é‡
# ä½†æ¨èä½¿ç”¨make_graph()å‡½æ•°æ¥è·å–å›¾å®ä¾‹
graph = None
